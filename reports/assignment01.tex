\documentclass[11pt,a4paper]{article}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{easy-todo}
\usepackage{amssymb}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=4cm]{geometry}
\usepackage[colorlinks,linkcolor=black,citecolor=black,filecolor=black]{hyperref}
\usepackage{listings}
\catcode`^=\active
\def^#1^{\texttt{#1}}


\begin{document}
\title{Autonomous Agents - Assignment 1}
\author{BasVeeling (10767770) \and Sebastian Droeppelmann (5783453) \and Fritjof Buettner (10876782)}
\maketitle
\listoftodos
\section{Introduction}
\subsection{Project structure}
We decided to model the predator/prey problem in Python. We chose an object-oriented structure to make the code reusable and minimize repetitions. In order to assure correct functionality of the algorithms at a fine-grained level, we make use of Python's unit-test framework.
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.6\textwidth]{classdiagram}
  \caption{Simulator Class diagram}
   \label{fig:classdiagram}
\end{figure}

The file \texttt{main.py} in the root directory of the project executes all the tasks required in the assignment and prints the according output to the console. The models for field, players and their respective policies are located in the subdirectory \texttt{models}. Their relationship is illustrated in figure~\ref{fig:classdiagram}. Both \texttt{Predator} and \texttt{Prey} class are children of the \texttt{Player} superclass, which provides common attributes such as the location and policy fields. 

There is a bidirectional connection between the field and the players on it, such that the field maintains a list of active players, who in return hold a reference to the field they are on in order to receive sensory information like the position of other players. Every player has their own policy (even the prey, which is not an agent at this point).
Furthermore, the field also provides the function \texttt{is\_ended()} which can be used to check whether the predator has caught the prey.

For a more human-readable output, the field implements the function \texttt{print\_field()} which prints an ASCII-graphical representation of the field to the console, with a \textbf{\texttt{X}} denoting the position of the predator(s) and a \textbf{\texttt{O}} denoting the position of the prey(s).

\section{Question 1: Environment Simulator}
We implemented the simulator as described above. The ^main.py^ ^as011()^ function runs one simulation with a random policy for both the prey and the predator. It initializes a 11x11 field with one predator and one prey. For every time step the predator performance an action based on its policy (^predator.act()^), and this will update the field, until the game is over (^field.is\_ended()^).

The ^main^ functions calls this simulation 100 times and calculates the average and stdev of the running time, this results in:
\begin{lstlisting}[language=bash]
$ python main.py
Mean runtime: 0.01544s (standard deviation: 0.0133)
Mean number of iterations: 1026.88 (standard deviation: 888.41)
\end{lstlisting}

\subsection{Random Policy}
The random policy of the predator is implemented in ^random\_predator\_policy.py^. The function ^get\_probability\_mapping(state)^ returns a mapping of probabilities to mappings, and the super class ^Policy^ uses this to pick a next action in the function \\^pick\_next\_action(state, style)^.


\section{Question 4: Value Iteration}
Value Iteration is implemented in ^value\_iteration.py^. The main part of the value iteration algorithm is implemented in the ^while go\_on:^ loop. The code is a translation of the algorithm explained in figure 4.5 of the book. Value calculation is done in  ^calculate\_value(\dots)^. This function is slightly cumbersome due to implementation choices made earlier, but properly calculates the value as prescribed in the algorithm. A heatmap of the values can be found in figure~\ref{fig:1}.

We compute the values for discount factors of $0.1, 0.5, 0.7$ and $0.9$. This results in the following output:
\begin{lstlisting}[language=bash]
Gamma = 0.1 took 6 iterations and 28.0588741302 seconds to converge.
Gamma = 0.5 took 7 iterations and 32.2343380451 seconds to converge.
Gamma = 0.7 took 7 iterations and 32.0498149395 seconds to converge.
Gamma = 0.9 took 7 iterations and 31.8123779297 seconds to converge.
\end{lstlisting}

\begin{figure}  
  \centering
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{valueiteration_gamma0-1}
    \caption{$\gamma$ = 0.1}\label{fig:1a}   
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{valueiteration_gamma0-5}
    \caption{$\gamma$ = 0.5}\label{fig:1b}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{valueiteration_gamma0-7}
    \caption{$\gamma$ = 0.7}\label{fig:1b}
  \end{subfigure}
  \quad
  \begin{subfigure}[t]{.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{valueiteration_gamma0-9}
    \caption{$\gamma$ = 0.9}\label{fig:1b}
  \end{subfigure}
  \caption{Heatmaps of the final values for prey(5,5), illustrating the effect of the discount factor $\lambda$}\label{fig:1}
\end{figure}
\section{Question 5: Relative Position}
We implemented a relative position in our code, but do to time constraints we were not able to get it working with the functions above. We considered encoding the state space as the Manhattan Distance from the Predator to the Prey. This reduces the size of the state space from $(11*11)**2$ = 14.641 to 10*10 = 100. 

We illustrate how the state would change, given an action of the predator and the stochastic movement of the prey in figure~\ref{fig:state_diagram}. Say the predator is at location 5,5, and the prey at 7,8 in the 11x11 grid. We would then encode the state as (2,3). If the predator then moves dow, and the prey would move up, the new state would be (0,3). 

One thing that needs to be considered when using this state encoding, is the toroidal grid. This means that a state (2,5) goes to (2,-5) when the predator moves left (and the prey stays).

\begin{figure}[h!]
    \includegraphics[width=0.6\textwidth]{state_diagram}
  \caption{State diagram for relative position}
   \label{fig:state_diagram}
\end{figure}
\section{Conclusion}
It's all great.
\end{document}